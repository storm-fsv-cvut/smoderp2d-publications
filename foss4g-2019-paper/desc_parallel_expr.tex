\subsection{Parallel computing experiments}

Because one of the most crucial points of SMODERP2D computations is
the speed, an experimental branch allowing (both CPU and GPU-based)
parallelised computations has been developed.

The main step was to rewrite all loop-based computations into
matrix-based mathematical operations. To keep matrices as so-called
tensors and to perform all the operations, an open source  TensorFlow Python
library \cite{tensorflow2015-whitepaper} developed by Google Brain
Team\footnote{https://ai.google/research/teams/brain/} was used. Even though
TensorFlow is most widely used for machine learning
and its performance on basic mathematical operations is not always better
than the one of NumPy (a quick comparison with NumPy and Numba can be
seen in \cite{tf-np}), it had been preferred for its easy switch
between CPU and GPU-based core (it depends only on the version of
TensorFlow the user has installed, no needs for changes in code) and
therefore support also for users without an access to machines with
GPU. Another advantage of TensorFlow is its usage of so-called
graphs. A graph is a representation of all operations in
dataflow/workflow and its individual operations are automatically sent
to multiple cores in a CPU or multiple threads in a GPU. These nodes
are run independently in parallel.

To support further development of TensorFlow and exploit its bleeding
edge functionalities, TensorFlow 2.0, which is published currently
just as an alpha version, was used in the SMODERP2D experimental
branch. Because TensorFlow 2.0 is still not suitable with all the
Python acrobatic tricks, NumPy was used for matrix operations in
places where TensorFlow could not (on places where loops were still
needed; looping through a NumPy array is incomparably faster than
through a Tensor).

This experimental SMODERP2D branch is still under development;
however, the alpha-version is ready to be used. Table \ref{tab:GPU_results}
presents the results of different tests made on this version.

\begin{table}[h]
  \centering
  \caption{Results of parallelization tests}
  \makegapedcells
  \begin{tabular}{|l|p{2.2cm}|c|c|}\hline
    RAM & Processing unit & Data 62 KB & Data 197 MB\\
     & & [s] & [s]\\\hline
    \multirow{2}{*}{15 GB} & GPU1 & 4.0 & 7 560\\
    & CPU1 & 0.2 & 12 809\\\hline
    \multirow{2}{*}{251 GB} & GPU2 & 2.5 & 6 631\\
    & CPU2 & 0.2 & 10 637\\\hline
  \end{tabular}
  \label{tab:GPU_results}
  \caption{Used processing units}
  \begin{tabular}{|l|p{1.9cm}|c|c|}\hline
    ID & Model & Clock speed & Memory\\\hline
    GPU1 & GeForce GTX 1060 3GB & 33 MHz & 3 016 MiB \\\hline
    GPU2 & 4x GeForce GTX 1080 Ti & 33 MHz & 11 178 MiB \\\hline
    CPU1 & AMD Ryzen 7 1700 Eight Core Processor & 1 373 MHz & 512 KB \\\hline
    CPU2 & Intel Xeon CPU E5-2630 v4 @ 2.20 GHz & 2 423 MHz & 25 600 KB \\\hline
  \end{tabular}
\end{table}

As can be seen in the table, the usage of GPUs is not always the right way.
The bottleneck of TensorFlow is its graph initialization; this step is very
time-consuming and therefore can last many times longer than the computation
itself for extremely small data. However, for data of common size and bigger
the computation was generally faster (reaching around 60 percent of the total
computation time on different architectures). Disappointing were results with
multiple GPUs - the code is going to be modified to exploit these
possibilities.

\subsubsection{Further ideas for a basins-based parallel computing}
Besides the GPU-based parallelization (with TensorFlow \cite{tensorflow2015-whitepaper} or
NVIDIA Cuda technology~\cite{Kalyanapu2011,Le2015}) the
CPU-parallelization may also bring a good improvement in the computation
time reduction. The computation domain is separated into sub-domains
based on certain algorithm were each sub-domain is loaded to single
CPU core. The domain may be separated into sub-domain with equal area
based on a defined grid. It is beneficial to incorporate also
hydrological behavior in the parallelization
strategy if the domain is a hydrological basin. 
In~\cite{Vivoni2011} the basin was separated in sub-basins
based on stream network. The sub-basins communicated with each other through so called
ghost cell. The strategy aimed to generate as less ghost cells as
possible.

The parallelization strategy outlined in the manuscript is based on
the hydrological reality and it is depicted in a simplified setup in
the Figure~\ref{fig:cpu-parallel}. In this example an Nučice experimental 
was chosen to present the parallelization strategy. At this 0.5 km$^2$ large 
basin a long tern monitoring of erosion and runoff processes is being conducted 
by the Dept.\ of Landscape Water Conservation. 

The strategy main goal is reduction of
the communication between CPU-cores during the computation as much as
possible. The whole basin is divided into several sub-basins based on
the digital elevation model and user defined sub-basin size. 
Outlet of each sub-basin is depicted with
red dots in the Figure~\ref{fig:cpu-parallel}. After the sub-basins
are defined, an order in which each sub-basin will be computed is defined as
follows. Sub-basins which are hydrologically the farthest from the
basin outlet (depicted by triangle in Figure~\ref{fig:cpu-parallel}) 
and therefore have no inflow flow upslope catchments are
calculated at first. Those sub-basins have the rainfall stored in
hyetographs the only input. In the simplified setup shown in the
Figure~\ref{fig:cpu-parallel}, the sub-basins 1, 2, 3, and 6 are
calculated at first in parallel. The calculated hydrographs of the
sub-basins 1, 2, 3, and 4 are stored in the memory for later. Sub-basins which
has an inflow from sub-basins 1, 2, 3, and 6 are calculated next. It
this case it is only the sub-basin 4. The water input in
the calculation of sub-basin 6 are now hyetograph and also hydrographs of
sub-basins 1 and 3. This the same manned are
calculated sub-basins towards in the whole basin outlet where its hydrograph 
is the desired outcome.

This approach may encounter in several limitations where the main one 
originates in the basin geometry. In case of narrow  basin a situation 
where each sub-basin has only one inflow and one outflow sub-basin may occur.
In this case, the computation sub-basins will be computed in parallel, which
lose the advantage of multi-core working station. If this situation happens
the user will be forced to create very small sub-basins in order to be able 
to performed the outlined CPU-parallelization. The possibilities of CPU-parallelization
outlined in this section will be the subject of further research.


\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=1.0\columnwidth]{figures/smoderp-cpu-parallel.png}
    \caption{Simple example of possible CPU-parallelization strategy for the experimental catchment Nučice}
    \label{fig:cpu-parallel}
  \end{center}
\end{figure}




